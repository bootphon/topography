{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification on CIFAR with a topographic ResNet18\n",
    "\n",
    "The following notebook trains a topographic ResNet18 from scratch on CIFAR-10\n",
    "and achieves 0.92 test accuracy after 100 epochs with the given\n",
    "hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from topography import TopographicLoss, TopographicModel\n",
    "from topography.models import resnet18\n",
    "from topography.training import Writer, evaluate, train\n",
    "from topography.utils import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0  # Random seed\n",
    "root = \"./cifar10_topo\"  # Output directory\n",
    "num_classes = 10  # Number of CIFAR classes. Must be 10 or 100\n",
    "epochs = 100  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "lr = 0.01  # Base learning rate\n",
    "weight_decay = 0.01  # Weight decay\n",
    "momentum = 0.9  # SGD momentum\n",
    "val_proportion = 0.1  # Proportion of the full train set for the validation set\n",
    "\n",
    "lambd = 0.1\n",
    "dimension = 2\n",
    "norm = \"euclidean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = (\n",
    "    torchvision.datasets.CIFAR10\n",
    "    if num_classes == 10\n",
    "    else torchvision.datasets.CIFAR100\n",
    ")\n",
    "\n",
    "train_set = dataset(\n",
    "    root=\"../../data\", train=True, download=True, transform=train_transform\n",
    ")\n",
    "\n",
    "val_set = dataset(\n",
    "    root=\"../../data\", train=True, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "num_train = len(train_set)\n",
    "indices = torch.randperm(num_train)\n",
    "split = int(np.floor(val_proportion * num_train))\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    sampler=val_sampler,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_set = dataset(\n",
    "    root=\"../../data\", train=False, download=True, transform=test_transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the main componenents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topography.core.loss import MetricOutput\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "topo_loss = TopographicLoss()\n",
    "\n",
    "model = TopographicModel(\n",
    "    resnet18(num_classes=num_classes), dimension=dimension, norm=norm\n",
    ").to(device)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    ")\n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer, warmup_epochs=epochs * 0.3, max_epochs=epochs\n",
    ")\n",
    "writer = Writer(f\"{root}/runs\")\n",
    "\n",
    "\n",
    "def criterion(output, target):\n",
    "    ce = cross_entropy(output, target)\n",
    "    topo = topo_loss(model.activations, model.inverse_distance)\n",
    "    return MetricOutput(\n",
    "        value=ce + lambd * topo.value,\n",
    "        extras={\n",
    "            \"loss-cross-entropy\": ce.item(),\n",
    "            \"loss-topographic\": topo.value.item(),\n",
    "            **topo.extras,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "writer.log_config(\n",
    "    dict(\n",
    "        num_classes=num_classes,\n",
    "        val_proportion=val_proportion,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        lambd=lambd,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=momentum,\n",
    "        optimizer=\"sgd\",\n",
    "        scheduler=\"LinearWarmupCosineAnnealingLR\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    train(model, train_loader, optimizer, criterion, device, writer)\n",
    "    evaluate(model, test_loader, criterion, device, writer, mode=\"val\")\n",
    "    scheduler.step()\n",
    "    writer.save(\n",
    "        \"val\", \"acc\", model=model, optimizer=optimizer, scheduler=scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader, criterion, device, writer, mode=\"test\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35e0b4034335b5c349218249c01092bcacbfaca87fce0d591779a5b0bb455cae"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('topo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
