{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification on CIFAR-10 with ResNet18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from topography.training import train, evaluate, Writer\n",
    "from topography.models import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 # Random seed\n",
    "root = './cifar10' # Output directory\n",
    "\n",
    "# epochs = 400 # Number of training epochs\n",
    "# batch_size = 256 # Batch size\n",
    "# lr = 0.1 # Base learning rate\n",
    "# weight_decay = 5e-3 # Weight decay\n",
    "# momentum = 0.9 # SGD momentum\n",
    "\n",
    "epochs = 100 # Number of training epochs\n",
    "batch_size = 256 # Batch size\n",
    "lr = 1e-2 # Base learning rate\n",
    "weight_decay = 1e-2 # Weight decay\n",
    "momentum = 0.9 # SGD momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=f'{root}/data', train=True, download=True, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=2, pin_memory=True)\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root=f'{root}/data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = resnet18(pretrained=False).to(device)\n",
    "# optimizer = optim.SGD(model.parameters(),\n",
    "#                       lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), nesterov=True,\n",
    "                      lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optimizer, milestones=[200, 300], gamma=0.1)\n",
    "from topography.models.scheduler import WarmupCosineLR\n",
    "scheduler = WarmupCosineLR(optimizer, warmup_epochs=epochs*0.3, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = Writer(f'{root}/runs')\n",
    "for _ in range(epochs):\n",
    "    train(model, train_loader, optimizer, criterion, device, writer)\n",
    "    evaluate(model, test_loader, criterion, device, writer, 'val')\n",
    "    scheduler.step()\n",
    "    writer.save('val', 'acc', model=model, optimizer=optimizer,\n",
    "                scheduler=scheduler)\n",
    "evaluate(model, test_loader, criterion, device, writer, 'test')\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35e0b4034335b5c349218249c01092bcacbfaca87fce0d591779a5b0bb455cae"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-topo]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
